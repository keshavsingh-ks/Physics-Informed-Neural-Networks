{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "nu = 0.01"
      ],
      "metadata": {
        "id": "VgJuHEmFwFsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NavierStokes():\n",
        "    def __init__(self, X, Y, T, u, v):\n",
        "        # Initialize inputs\n",
        "        self.x = torch.tensor(X, dtype=torch.float32, requires_grad=True)\n",
        "        self.y = torch.tensor(Y, dtype=torch.float32, requires_grad=True)\n",
        "        self.t = torch.tensor(T, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "        self.u = torch.tensor(u, dtype=torch.float32)\n",
        "        self.v = torch.tensor(v, dtype=torch.float32)\n",
        "        self.null = torch.zeros((self.x.shape[0], 1))\n",
        "\n",
        "        self.network()\n",
        "\n",
        "        self.optimizer = torch.optim.LBFGS(self.net.parameters(), lr=0.1, max_iter=10000, max_eval=8000,\n",
        "                                   history_size=50, tolerance_grad=1e-05, tolerance_change=0.5 * np.finfo(float).eps,\n",
        "                                   line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "        self.ls = 0\n",
        "        self.iter = 0\n",
        "\n",
        "    def network(self):\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 2))\n",
        "\n",
        "\n",
        "    def function(self, x, y, t):\n",
        "        res = self.net(torch.hstack((x, y, t)))\n",
        "        psi, p = res[:, 0:1], res[:, 1:2]\n",
        "        u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
        "        v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
        "\n",
        "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "\n",
        "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "\n",
        "        p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "        p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "\n",
        "        f = u_t + u * u_x + v * u_y + p_x - nu * (torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0] +\n",
        "                                                  torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0])\n",
        "        g = v_t + u * v_x + v * v_y + p_y - nu * (torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0] +\n",
        "                                                  torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0])\n",
        "\n",
        "        return u, v, p, f, g\n",
        "\n",
        "    def closure(self):\n",
        "      self.optimizer.zero_grad()\n",
        "      u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = self.function(self.x, self.y, self.t)\n",
        "\n",
        "      u_loss = self.mse(u_prediction, self.u)  # Error between predicted and real u values\n",
        "      v_loss = self.mse(v_prediction, self.v)  # Error between predicted and real v values\n",
        "      f_loss = self.mse(f_prediction, self.null)  # Enforce Navier-Stokes residual for horizontal direction\n",
        "      g_loss = self.mse(g_prediction, self.null)  # Enforce Navier-Stokes residual for vertical direction\n",
        "\n",
        "\n",
        "      self.ls = u_loss + v_loss + f_loss + g_loss\n",
        "      self.ls.backward()\n",
        "\n",
        "      self.iter += 1\n",
        "      if not self.iter % 100:\n",
        "          print('Iteration: {:}, Loss: {:0.6f}'.format(self.iter, self.ls.item()))\n",
        "\n",
        "      return self.ls\n",
        "\n",
        "    def train(self):\n",
        "      self.net.train()\n",
        "\n",
        "      self.optimizer.step(self.closure)\n",
        "\n"
      ],
      "metadata": {
        "id": "vQP4cPGk8bT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 5000\n",
        "\n",
        "data = scipy.io.loadmat('cylinder_wake.mat')"
      ],
      "metadata": {
        "id": "vwz2H1YWBB15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_star = data['X_star']  # Spatial coordinates (N x 2)\n",
        "U_star = data['U_star']  # Velocities (N x 2 x T)\n",
        "P_star = data['p_star']  # Pressure (N x T)\n",
        "t_star = data['t']  # Time (T x 1)\n",
        "\n",
        "N = X_star.shape[0]\n",
        "T = t_star.shape[0]\n",
        "XX = np.tile(X_star[:, 0:1], (1, T)).flatten()[:, None]\n",
        "YY = np.tile(X_star[:, 1:2], (1, T)).flatten()[:, None]\n",
        "TT = np.tile(t_star, (1, N)).T.flatten()[:, None]\n",
        "UU = U_star[:, 0, :].flatten()[:, None]\n",
        "VV = U_star[:, 1, :].flatten()[:, None]\n",
        "\n",
        "idx = np.random.choice(N * T, N_train, replace=False)\n",
        "x_train = XX[idx, :]\n",
        "y_train = YY[idx, :]\n",
        "t_train = TT[idx, :]\n",
        "u_train = UU[idx, :]\n",
        "v_train = VV[idx, :]\n"
      ],
      "metadata": {
        "id": "xPNeFx-KBDwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinn = NavierStokes(x_train, y_train, t_train, u_train, v_train)\n",
        "pinn.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GL_fSK_B-dh",
        "outputId": "3a264848-825a-49d8-c77e-dc894c132101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100, Loss: 0.095579\n",
            "Iteration: 200, Loss: 0.079807\n",
            "Iteration: 300, Loss: 0.073595\n",
            "Iteration: 400, Loss: 0.069768\n",
            "Iteration: 500, Loss: 0.067365\n",
            "Iteration: 600, Loss: 0.065033\n",
            "Iteration: 700, Loss: 0.063282\n",
            "Iteration: 800, Loss: 0.062160\n",
            "Iteration: 900, Loss: 0.060910\n",
            "Iteration: 1000, Loss: 0.059812\n",
            "Iteration: 1100, Loss: 0.058789\n",
            "Iteration: 1200, Loss: 0.057949\n",
            "Iteration: 1300, Loss: 0.057146\n",
            "Iteration: 1400, Loss: 0.056376\n",
            "Iteration: 1500, Loss: 0.055568\n",
            "Iteration: 1600, Loss: 0.053827\n",
            "Iteration: 1700, Loss: 0.052647\n",
            "Iteration: 1800, Loss: 0.051371\n",
            "Iteration: 1900, Loss: 0.049742\n",
            "Iteration: 2000, Loss: 0.048419\n",
            "Iteration: 2100, Loss: 0.047137\n",
            "Iteration: 2200, Loss: 0.045987\n",
            "Iteration: 2300, Loss: 0.044948\n",
            "Iteration: 2400, Loss: 0.043786\n",
            "Iteration: 2500, Loss: 0.042923\n",
            "Iteration: 2600, Loss: 0.041758\n",
            "Iteration: 2700, Loss: 0.040711\n",
            "Iteration: 2800, Loss: 0.039791\n",
            "Iteration: 2900, Loss: 0.038974\n",
            "Iteration: 3000, Loss: 0.038287\n",
            "Iteration: 3100, Loss: 0.037392\n",
            "Iteration: 3200, Loss: 0.036497\n",
            "Iteration: 3300, Loss: 0.035149\n",
            "Iteration: 3400, Loss: 0.034443\n",
            "Iteration: 3500, Loss: 0.033867\n",
            "Iteration: 3600, Loss: 0.033248\n",
            "Iteration: 3700, Loss: 0.032459\n",
            "Iteration: 3800, Loss: 0.031681\n",
            "Iteration: 3900, Loss: 0.031316\n",
            "Iteration: 4000, Loss: 0.030834\n",
            "Iteration: 4100, Loss: 0.030492\n",
            "Iteration: 4200, Loss: 0.029928\n",
            "Iteration: 4300, Loss: 0.029527\n",
            "Iteration: 4400, Loss: 0.029053\n",
            "Iteration: 4500, Loss: 0.028722\n",
            "Iteration: 4600, Loss: 0.028271\n",
            "Iteration: 4700, Loss: 0.028046\n",
            "Iteration: 4800, Loss: 0.027744\n",
            "Iteration: 4900, Loss: 0.027502\n",
            "Iteration: 5000, Loss: 0.027195\n",
            "Iteration: 5100, Loss: 0.026744\n",
            "Iteration: 5200, Loss: 0.026330\n",
            "Iteration: 5300, Loss: 0.025796\n",
            "Iteration: 5400, Loss: 0.025497\n",
            "Iteration: 5500, Loss: 0.025210\n",
            "Iteration: 5600, Loss: 0.024838\n",
            "Iteration: 5700, Loss: 0.024492\n",
            "Iteration: 5800, Loss: 0.024170\n",
            "Iteration: 5900, Loss: 0.023861\n",
            "Iteration: 6000, Loss: 0.023575\n",
            "Iteration: 6100, Loss: 0.023352\n",
            "Iteration: 6200, Loss: 0.023146\n",
            "Iteration: 6300, Loss: 0.022944\n",
            "Iteration: 6400, Loss: 0.022751\n",
            "Iteration: 6500, Loss: 0.022528\n",
            "Iteration: 6600, Loss: 0.022351\n",
            "Iteration: 6700, Loss: 0.022180\n",
            "Iteration: 6800, Loss: 0.022009\n",
            "Iteration: 6900, Loss: 0.021841\n",
            "Iteration: 7000, Loss: 0.021644\n",
            "Iteration: 7100, Loss: 0.021475\n",
            "Iteration: 7200, Loss: 0.021294\n",
            "Iteration: 7300, Loss: 0.021095\n",
            "Iteration: 7400, Loss: 0.020872\n",
            "Iteration: 7500, Loss: 0.020730\n",
            "Iteration: 7600, Loss: 0.020524\n",
            "Iteration: 7700, Loss: 0.020319\n",
            "Iteration: 7800, Loss: 0.020222\n",
            "Iteration: 7900, Loss: 0.020099\n",
            "Iteration: 8000, Loss: 0.019919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = X_star[:, 0:1]\n",
        "y_test = X_star[:, 1:2]\n",
        "t_test = np.ones((x_test.shape[0], x_test.shape[1]))  # Let's assume we're evaluating at time t=1 for simplicity\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32, requires_grad=True)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32, requires_grad=True)\n",
        "t_test = torch.tensor(t_test, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "\n",
        "u_out, v_out, p_out, f_out, g_out = pinn.function(x_test, y_test, t_test)\n"
      ],
      "metadata": {
        "id": "6ylpG1B4FeYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_plot = p_out.data.cpu().numpy()\n",
        "p_plot = np.reshape(p_plot, (50, 100))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.contourf(p_plot, levels=30, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.title(r'$p(x, y, t)$ - Predicted Pressure')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "zRbjp-mfFfOV",
        "outputId": "2f0e92b6-f2a0-4259-8350-0e60ade23b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "u_plot = u_out.data.cpu().numpy()\n",
        "v_plot = v_out.data.cpu().numpy()\n",
        "f_plot = f_out.data.cpu().numpy()\n",
        "g_plot = g_out.data.cpu().numpy()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(u_plot)\n",
        "plt.title('u')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(v_plot)\n",
        "plt.title('v')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(f_plot)\n",
        "plt.title('f')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(g_plot)\n",
        "plt.title('g')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6wlFdpbZPFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transient flow of data"
      ],
      "metadata": {
        "id": "bu6jUrn-G029"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = scipy.io.loadmat('cylinder_wake.mat')\n",
        "\n",
        "# Inspect the data keys to see available variables\n",
        "print(data.keys())\n",
        "\n",
        "# Extract relevant information from the dataset\n",
        "U_star = data['U_star']  # Velocity components (N x 2 x T)\n",
        "P_star = data['p_star']  # Pressure (N x T)\n",
        "t_star = data['t']       # Time points (T x 1)\n",
        "X_star = data['X_star']  # Spatial coordinates (N x 2)\n",
        "\n",
        "print(\"Velocity data shape:\", U_star.shape)\n",
        "print(\"Pressure data shape:\", P_star.shape)\n",
        "print(\"Time points shape:\", t_star.shape)\n",
        "print(\"Spatial coordinates shape:\", X_star.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4_Qq7bfG4ks",
        "outputId": "0bf16918-393f-4414-ed74-44caef02ee03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'X_star', 't', 'U_star', 'p_star'])\n",
            "Velocity data shape: (5000, 2, 200)\n",
            "Pressure data shape: (5000, 200)\n",
            "Time points shape: (200, 1)\n",
            "Spatial coordinates shape: (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_star = X_star[:, 0:1]  # x coordinates (N x 1)\n",
        "y_star = X_star[:, 1:2]  # y coordinates (N x 1)\n",
        "t_star = t_star.flatten()  # Time points (T, )\n",
        "\n",
        "N = x_star.shape[0]\n",
        "T = len(t_star)\n",
        "\n",
        "XX = np.tile(x_star, (1, T))  # N x T\n",
        "YY = np.tile(y_star, (1, T))  # N x T\n",
        "TT = np.tile(t_star, (N, 1))  # N x T\n",
        "\n",
        "UU = U_star[:, 0, :]  # x-velocity (N x T)\n",
        "VV = U_star[:, 1, :]  # y-velocity (N x T)\n",
        "PP = P_star           # Pressure (N x T)\n",
        "\n",
        "x = XX.flatten()[:, None]\n",
        "y = YY.flatten()[:, None]\n",
        "t = TT.flatten()[:, None]\n",
        "u = UU.flatten()[:, None]\n",
        "v = VV.flatten()[:, None]\n",
        "p = PP.flatten()[:, None]\n",
        "\n",
        "N_train = 5000  # You can adjust this number for faster training\n",
        "idx = np.random.choice(N * T, N_train, replace=False)\n",
        "\n",
        "x_train = x[idx, :]\n",
        "y_train = y[idx, :]\n",
        "t_train = t[idx, :]\n",
        "u_train = u[idx, :]\n",
        "v_train = v[idx, :]\n",
        "\n",
        "print(f\"Number of training samples: {x_train.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfINfRuGG7zJ",
        "outputId": "1416c729-13ec-433d-94d3-116136f2e46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransientNavierStokes():\n",
        "    def __init__(self, X, Y, T, u, v):\n",
        "        self.x = torch.tensor(X, dtype=torch.float32, requires_grad=True)\n",
        "        self.y = torch.tensor(Y, dtype=torch.float32, requires_grad=True)\n",
        "        self.t = torch.tensor(T, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "        self.u = torch.tensor(u, dtype=torch.float32)\n",
        "        self.v = torch.tensor(v, dtype=torch.float32)\n",
        "\n",
        "        self.null = torch.zeros((self.x.shape[0], 1))\n",
        "\n",
        "        self.network()\n",
        "\n",
        "        self.optimizer = torch.optim.LBFGS(self.net.parameters(), lr=0.01, max_iter=10000, max_eval=8000,\n",
        "                                           history_size=50, tolerance_grad=1e-05,\n",
        "                                           tolerance_change=0.5 * np.finfo(float).eps,\n",
        "                                           line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.ls = 0\n",
        "        self.iter = 0\n",
        "\n",
        "    def network(self):\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 20), nn.Tanh(),\n",
        "            nn.Linear(20, 3))  # Output: [u, v, p]\n",
        "\n",
        "    def function(self, x, y, t):\n",
        "        res = self.net(torch.hstack((x, y, t)))\n",
        "        u, v, p = res[:, 0:1], res[:, 1:2], res[:, 2:3]\n",
        "\n",
        "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "\n",
        "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "\n",
        "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "\n",
        "        p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "        p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "\n",
        "\n",
        "        f = u_t + u * u_x + v * u_y + p_x - nu * (u_x + u_y)\n",
        "        g = v_t + u * v_x + v * v_y + p_y - nu * (v_x + v_y)\n",
        "\n",
        "        return u, v, p, f, g\n",
        "\n",
        "\n",
        "    def closure(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = self.function(self.x, self.y, self.t)\n",
        "\n",
        "        u_loss = self.mse(u_prediction, self.u)\n",
        "        v_loss = self.mse(v_prediction, self.v)\n",
        "\n",
        "        f_loss = self.mse(f_prediction, self.null)\n",
        "        g_loss = self.mse(g_prediction, self.null)\n",
        "\n",
        "        self.ls = u_loss + v_loss + f_loss + g_loss\n",
        "\n",
        "\n",
        "        self.ls.backward()\n",
        "\n",
        "\n",
        "        self.iter += 1\n",
        "        if not self.iter % 10:  # Print every 10 iterations\n",
        "            print(f'Iteration: {self.iter}, Loss: {self.ls.item():.6f}')\n",
        "\n",
        "        return self.ls\n",
        "\n",
        "    def train(self):\n",
        "        self.net.train()\n",
        "        self.optimizer.step(self.closure)\n"
      ],
      "metadata": {
        "id": "ynOHpinJHMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinn_transient = TransientNavierStokes(x_train, y_train, t_train, u_train, v_train)\n",
        "\n",
        "\n",
        "pinn_transient.train()\n",
        "\n",
        "\n",
        "torch.save(pinn_transient.net.state_dict(), 'transient_model.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbtKiOlHaGP",
        "outputId": "2341857f-e195-4992-d0b5-c2807a63f005",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10, Loss: 0.156012\n",
            "Iteration: 20, Loss: 0.155991\n",
            "Iteration: 30, Loss: 0.155983\n",
            "Iteration: 40, Loss: 0.155979\n",
            "Iteration: 50, Loss: 0.155958\n",
            "Iteration: 60, Loss: 0.155787\n",
            "Iteration: 70, Loss: 0.155466\n",
            "Iteration: 80, Loss: 0.155400\n",
            "Iteration: 90, Loss: 0.155377\n",
            "Iteration: 100, Loss: 0.155343\n",
            "Iteration: 110, Loss: 0.155039\n",
            "Iteration: 120, Loss: 0.154558\n",
            "Iteration: 130, Loss: 0.154074\n",
            "Iteration: 140, Loss: 0.153428\n",
            "Iteration: 150, Loss: 0.150727\n",
            "Iteration: 160, Loss: 0.145012\n",
            "Iteration: 170, Loss: 0.134181\n",
            "Iteration: 180, Loss: 0.128133\n",
            "Iteration: 190, Loss: 0.123972\n",
            "Iteration: 200, Loss: 0.118893\n",
            "Iteration: 210, Loss: 0.116665\n",
            "Iteration: 220, Loss: 0.114250\n",
            "Iteration: 230, Loss: 0.112777\n",
            "Iteration: 240, Loss: 0.109416\n",
            "Iteration: 250, Loss: 0.107290\n",
            "Iteration: 260, Loss: 0.103623\n",
            "Iteration: 270, Loss: 0.102096\n",
            "Iteration: 280, Loss: 0.101356\n",
            "Iteration: 290, Loss: 0.100638\n",
            "Iteration: 300, Loss: 0.100160\n",
            "Iteration: 310, Loss: 0.099785\n",
            "Iteration: 320, Loss: 0.099463\n",
            "Iteration: 330, Loss: 0.099355\n",
            "Iteration: 340, Loss: 0.099286\n",
            "Iteration: 350, Loss: 0.099153\n",
            "Iteration: 360, Loss: 0.099007\n",
            "Iteration: 370, Loss: 0.098943\n",
            "Iteration: 380, Loss: 0.098858\n",
            "Iteration: 390, Loss: 0.098772\n",
            "Iteration: 400, Loss: 0.098642\n",
            "Iteration: 410, Loss: 0.098424\n",
            "Iteration: 420, Loss: 0.098351\n",
            "Iteration: 430, Loss: 0.098295\n",
            "Iteration: 440, Loss: 0.098181\n",
            "Iteration: 450, Loss: 0.098047\n",
            "Iteration: 460, Loss: 0.097979\n",
            "Iteration: 470, Loss: 0.097944\n",
            "Iteration: 480, Loss: 0.097898\n",
            "Iteration: 490, Loss: 0.097809\n",
            "Iteration: 500, Loss: 0.097713\n",
            "Iteration: 510, Loss: 0.097565\n",
            "Iteration: 520, Loss: 0.097426\n",
            "Iteration: 530, Loss: 0.097089\n",
            "Iteration: 540, Loss: 0.096919\n",
            "Iteration: 550, Loss: 0.096656\n",
            "Iteration: 560, Loss: 0.096500\n",
            "Iteration: 570, Loss: 0.096209\n",
            "Iteration: 580, Loss: 0.096033\n",
            "Iteration: 590, Loss: 0.095824\n",
            "Iteration: 600, Loss: 0.095558\n",
            "Iteration: 610, Loss: 0.095373\n",
            "Iteration: 620, Loss: 0.095038\n",
            "Iteration: 630, Loss: 0.094337\n",
            "Iteration: 640, Loss: 0.093650\n",
            "Iteration: 650, Loss: 0.092371\n",
            "Iteration: 660, Loss: 0.091582\n",
            "Iteration: 670, Loss: 0.091130\n",
            "Iteration: 680, Loss: 0.090494\n",
            "Iteration: 690, Loss: 0.089598\n",
            "Iteration: 700, Loss: 0.089066\n",
            "Iteration: 710, Loss: 0.088555\n",
            "Iteration: 720, Loss: 0.088187\n",
            "Iteration: 730, Loss: 0.087778\n",
            "Iteration: 740, Loss: 0.087043\n",
            "Iteration: 750, Loss: 0.086682\n",
            "Iteration: 760, Loss: 0.086128\n",
            "Iteration: 770, Loss: 0.085323\n",
            "Iteration: 780, Loss: 0.084847\n",
            "Iteration: 790, Loss: 0.084324\n",
            "Iteration: 800, Loss: 0.084019\n",
            "Iteration: 810, Loss: 0.083904\n",
            "Iteration: 820, Loss: 0.083686\n",
            "Iteration: 830, Loss: 0.083327\n",
            "Iteration: 840, Loss: 0.083126\n",
            "Iteration: 850, Loss: 0.082768\n",
            "Iteration: 860, Loss: 0.082373\n",
            "Iteration: 870, Loss: 0.082182\n",
            "Iteration: 880, Loss: 0.081889\n",
            "Iteration: 890, Loss: 0.081662\n",
            "Iteration: 900, Loss: 0.081465\n",
            "Iteration: 910, Loss: 0.081236\n",
            "Iteration: 920, Loss: 0.081087\n",
            "Iteration: 930, Loss: 0.080901\n",
            "Iteration: 940, Loss: 0.080680\n",
            "Iteration: 950, Loss: 0.080564\n",
            "Iteration: 960, Loss: 0.080358\n",
            "Iteration: 970, Loss: 0.080159\n",
            "Iteration: 980, Loss: 0.080044\n",
            "Iteration: 990, Loss: 0.079888\n",
            "Iteration: 1000, Loss: 0.079720\n",
            "Iteration: 1010, Loss: 0.079615\n",
            "Iteration: 1020, Loss: 0.079332\n",
            "Iteration: 1030, Loss: 0.079238\n",
            "Iteration: 1040, Loss: 0.079146\n",
            "Iteration: 1050, Loss: 0.079006\n",
            "Iteration: 1060, Loss: 0.078842\n",
            "Iteration: 1070, Loss: 0.078548\n",
            "Iteration: 1080, Loss: 0.078391\n",
            "Iteration: 1090, Loss: 0.078293\n",
            "Iteration: 1100, Loss: 0.078216\n",
            "Iteration: 1110, Loss: 0.078133\n",
            "Iteration: 1120, Loss: 0.077885\n",
            "Iteration: 1130, Loss: 0.077688\n",
            "Iteration: 1140, Loss: 0.077576\n",
            "Iteration: 1150, Loss: 0.077392\n",
            "Iteration: 1160, Loss: 0.077186\n",
            "Iteration: 1170, Loss: 0.076917\n",
            "Iteration: 1180, Loss: 0.076562\n",
            "Iteration: 1190, Loss: 0.076424\n",
            "Iteration: 1200, Loss: 0.076280\n",
            "Iteration: 1210, Loss: 0.076093\n",
            "Iteration: 1220, Loss: 0.075854\n",
            "Iteration: 1230, Loss: 0.075665\n",
            "Iteration: 1240, Loss: 0.075515\n",
            "Iteration: 1250, Loss: 0.075334\n",
            "Iteration: 1260, Loss: 0.075192\n",
            "Iteration: 1270, Loss: 0.074970\n",
            "Iteration: 1280, Loss: 0.074682\n",
            "Iteration: 1290, Loss: 0.074129\n",
            "Iteration: 1300, Loss: 0.073812\n",
            "Iteration: 1310, Loss: 0.073547\n",
            "Iteration: 1320, Loss: 0.073326\n",
            "Iteration: 1330, Loss: 0.072974\n",
            "Iteration: 1340, Loss: 0.072681\n",
            "Iteration: 1350, Loss: 0.072268\n",
            "Iteration: 1360, Loss: 0.072071\n",
            "Iteration: 1370, Loss: 0.071826\n",
            "Iteration: 1380, Loss: 0.071622\n",
            "Iteration: 1390, Loss: 0.071446\n",
            "Iteration: 1400, Loss: 0.071176\n",
            "Iteration: 1410, Loss: 0.071014\n",
            "Iteration: 1420, Loss: 0.070705\n",
            "Iteration: 1430, Loss: 0.070553\n",
            "Iteration: 1440, Loss: 0.070376\n",
            "Iteration: 1450, Loss: 0.070129\n",
            "Iteration: 1460, Loss: 0.069856\n",
            "Iteration: 1470, Loss: 0.069458\n",
            "Iteration: 1480, Loss: 0.069201\n",
            "Iteration: 1490, Loss: 0.068953\n",
            "Iteration: 1500, Loss: 0.068757\n",
            "Iteration: 1510, Loss: 0.068214\n",
            "Iteration: 1520, Loss: 0.068045\n",
            "Iteration: 1530, Loss: 0.067837\n",
            "Iteration: 1540, Loss: 0.067293\n",
            "Iteration: 1550, Loss: 0.066863\n",
            "Iteration: 1560, Loss: 0.066442\n",
            "Iteration: 1570, Loss: 0.065815\n",
            "Iteration: 1580, Loss: 0.065640\n",
            "Iteration: 1590, Loss: 0.065480\n",
            "Iteration: 1600, Loss: 0.065269\n",
            "Iteration: 1610, Loss: 0.064972\n",
            "Iteration: 1620, Loss: 0.064555\n",
            "Iteration: 1630, Loss: 0.063840\n",
            "Iteration: 1640, Loss: 0.063555\n",
            "Iteration: 1650, Loss: 0.063402\n",
            "Iteration: 1660, Loss: 0.063028\n",
            "Iteration: 1670, Loss: 0.062694\n",
            "Iteration: 1680, Loss: 0.062358\n",
            "Iteration: 1690, Loss: 0.061815\n",
            "Iteration: 1700, Loss: 0.061476\n",
            "Iteration: 1710, Loss: 0.061045\n",
            "Iteration: 1720, Loss: 0.060801\n",
            "Iteration: 1730, Loss: 0.060471\n",
            "Iteration: 1740, Loss: 0.060059\n",
            "Iteration: 1750, Loss: 0.059790\n",
            "Iteration: 1760, Loss: 0.059520\n",
            "Iteration: 1770, Loss: 0.059194\n",
            "Iteration: 1780, Loss: 0.058762\n",
            "Iteration: 1790, Loss: 0.058526\n",
            "Iteration: 1800, Loss: 0.058128\n",
            "Iteration: 1810, Loss: 0.057726\n",
            "Iteration: 1820, Loss: 0.057257\n",
            "Iteration: 1830, Loss: 0.057002\n",
            "Iteration: 1840, Loss: 0.056693\n",
            "Iteration: 1850, Loss: 0.056404\n",
            "Iteration: 1860, Loss: 0.056112\n",
            "Iteration: 1870, Loss: 0.055876\n",
            "Iteration: 1880, Loss: 0.055761\n",
            "Iteration: 1890, Loss: 0.055485\n",
            "Iteration: 1900, Loss: 0.055306\n",
            "Iteration: 1910, Loss: 0.055052\n",
            "Iteration: 1920, Loss: 0.054919\n",
            "Iteration: 1930, Loss: 0.054806\n",
            "Iteration: 1940, Loss: 0.054655\n",
            "Iteration: 1950, Loss: 0.054451\n",
            "Iteration: 1960, Loss: 0.054336\n",
            "Iteration: 1970, Loss: 0.054197\n",
            "Iteration: 1980, Loss: 0.054094\n",
            "Iteration: 1990, Loss: 0.053951\n",
            "Iteration: 2000, Loss: 0.053766\n",
            "Iteration: 2010, Loss: 0.053662\n",
            "Iteration: 2020, Loss: 0.053508\n",
            "Iteration: 2030, Loss: 0.053443\n",
            "Iteration: 2040, Loss: 0.053383\n",
            "Iteration: 2050, Loss: 0.053278\n",
            "Iteration: 2060, Loss: 0.053107\n",
            "Iteration: 2070, Loss: 0.053011\n",
            "Iteration: 2080, Loss: 0.052862\n",
            "Iteration: 2090, Loss: 0.052718\n",
            "Iteration: 2100, Loss: 0.052601\n",
            "Iteration: 2110, Loss: 0.052444\n",
            "Iteration: 2120, Loss: 0.052354\n",
            "Iteration: 2130, Loss: 0.052211\n",
            "Iteration: 2140, Loss: 0.052078\n",
            "Iteration: 2150, Loss: 0.051974\n",
            "Iteration: 2160, Loss: 0.051879\n",
            "Iteration: 2170, Loss: 0.051767\n",
            "Iteration: 2180, Loss: 0.051684\n",
            "Iteration: 2190, Loss: 0.051635\n",
            "Iteration: 2200, Loss: 0.051544\n",
            "Iteration: 2210, Loss: 0.051491\n",
            "Iteration: 2220, Loss: 0.051276\n",
            "Iteration: 2230, Loss: 0.051189\n",
            "Iteration: 2240, Loss: 0.051115\n",
            "Iteration: 2250, Loss: 0.051047\n",
            "Iteration: 2260, Loss: 0.050987\n",
            "Iteration: 2270, Loss: 0.050929\n",
            "Iteration: 2280, Loss: 0.050813\n",
            "Iteration: 2290, Loss: 0.050723\n",
            "Iteration: 2300, Loss: 0.050643\n",
            "Iteration: 2310, Loss: 0.050547\n",
            "Iteration: 2320, Loss: 0.050488\n",
            "Iteration: 2330, Loss: 0.050449\n",
            "Iteration: 2340, Loss: 0.050410\n",
            "Iteration: 2350, Loss: 0.050364\n",
            "Iteration: 2360, Loss: 0.050318\n",
            "Iteration: 2370, Loss: 0.050222\n",
            "Iteration: 2380, Loss: 0.050175\n",
            "Iteration: 2390, Loss: 0.050101\n",
            "Iteration: 2400, Loss: 0.049952\n",
            "Iteration: 2410, Loss: 0.049861\n",
            "Iteration: 2420, Loss: 0.049752\n",
            "Iteration: 2430, Loss: 0.049619\n",
            "Iteration: 2440, Loss: 0.049547\n",
            "Iteration: 2450, Loss: 0.049472\n",
            "Iteration: 2460, Loss: 0.049409\n",
            "Iteration: 2470, Loss: 0.049360\n",
            "Iteration: 2480, Loss: 0.049304\n",
            "Iteration: 2490, Loss: 0.049205\n",
            "Iteration: 2500, Loss: 0.049150\n",
            "Iteration: 2510, Loss: 0.049071\n",
            "Iteration: 2520, Loss: 0.048978\n",
            "Iteration: 2530, Loss: 0.048881\n",
            "Iteration: 2540, Loss: 0.048773\n",
            "Iteration: 2550, Loss: 0.048699\n",
            "Iteration: 2560, Loss: 0.048525\n",
            "Iteration: 2570, Loss: 0.048471\n",
            "Iteration: 2580, Loss: 0.048416\n",
            "Iteration: 2590, Loss: 0.048320\n",
            "Iteration: 2600, Loss: 0.048209\n",
            "Iteration: 2610, Loss: 0.048125\n",
            "Iteration: 2620, Loss: 0.048077\n",
            "Iteration: 2630, Loss: 0.048000\n",
            "Iteration: 2640, Loss: 0.047943\n",
            "Iteration: 2650, Loss: 0.047817\n",
            "Iteration: 2660, Loss: 0.047765\n",
            "Iteration: 2670, Loss: 0.047708\n",
            "Iteration: 2680, Loss: 0.047618\n",
            "Iteration: 2690, Loss: 0.047553\n",
            "Iteration: 2700, Loss: 0.047464\n",
            "Iteration: 2710, Loss: 0.047410\n",
            "Iteration: 2720, Loss: 0.047380\n",
            "Iteration: 2730, Loss: 0.047290\n",
            "Iteration: 2740, Loss: 0.047261\n",
            "Iteration: 2750, Loss: 0.047205\n",
            "Iteration: 2760, Loss: 0.047109\n",
            "Iteration: 2770, Loss: 0.047031\n",
            "Iteration: 2780, Loss: 0.046967\n",
            "Iteration: 2790, Loss: 0.046885\n",
            "Iteration: 2800, Loss: 0.046820\n",
            "Iteration: 2810, Loss: 0.046748\n",
            "Iteration: 2820, Loss: 0.046651\n",
            "Iteration: 2830, Loss: 0.046601\n",
            "Iteration: 2840, Loss: 0.046555\n",
            "Iteration: 2850, Loss: 0.046507\n",
            "Iteration: 2860, Loss: 0.046441\n",
            "Iteration: 2870, Loss: 0.046381\n",
            "Iteration: 2880, Loss: 0.046338\n",
            "Iteration: 2890, Loss: 0.046286\n",
            "Iteration: 2900, Loss: 0.046214\n",
            "Iteration: 2910, Loss: 0.046131\n",
            "Iteration: 2920, Loss: 0.046075\n",
            "Iteration: 2930, Loss: 0.046034\n",
            "Iteration: 2940, Loss: 0.045971\n",
            "Iteration: 2950, Loss: 0.045886\n",
            "Iteration: 2960, Loss: 0.045831\n",
            "Iteration: 2970, Loss: 0.045780\n",
            "Iteration: 2980, Loss: 0.045727\n",
            "Iteration: 2990, Loss: 0.045644\n",
            "Iteration: 3000, Loss: 0.045496\n",
            "Iteration: 3010, Loss: 0.045412\n",
            "Iteration: 3020, Loss: 0.045284\n",
            "Iteration: 3030, Loss: 0.045218\n",
            "Iteration: 3040, Loss: 0.045094\n",
            "Iteration: 3050, Loss: 0.044966\n",
            "Iteration: 3060, Loss: 0.044772\n",
            "Iteration: 3070, Loss: 0.044690\n",
            "Iteration: 3080, Loss: 0.044608\n",
            "Iteration: 3090, Loss: 0.044505\n",
            "Iteration: 3100, Loss: 0.044369\n",
            "Iteration: 3110, Loss: 0.044319\n",
            "Iteration: 3120, Loss: 0.044254\n",
            "Iteration: 3130, Loss: 0.044184\n",
            "Iteration: 3140, Loss: 0.044137\n",
            "Iteration: 3150, Loss: 0.044042\n",
            "Iteration: 3160, Loss: 0.043953\n",
            "Iteration: 3170, Loss: 0.043867\n",
            "Iteration: 3180, Loss: 0.043766\n",
            "Iteration: 3190, Loss: 0.043682\n",
            "Iteration: 3200, Loss: 0.043609\n",
            "Iteration: 3210, Loss: 0.043457\n",
            "Iteration: 3220, Loss: 0.043393\n",
            "Iteration: 3230, Loss: 0.043357\n",
            "Iteration: 3240, Loss: 0.043274\n",
            "Iteration: 3250, Loss: 0.043249\n",
            "Iteration: 3260, Loss: 0.043209\n",
            "Iteration: 3270, Loss: 0.043137\n",
            "Iteration: 3280, Loss: 0.042998\n",
            "Iteration: 3290, Loss: 0.042863\n",
            "Iteration: 3300, Loss: 0.042808\n",
            "Iteration: 3310, Loss: 0.042656\n",
            "Iteration: 3320, Loss: 0.042549\n",
            "Iteration: 3330, Loss: 0.042448\n",
            "Iteration: 3340, Loss: 0.042231\n",
            "Iteration: 3350, Loss: 0.042103\n",
            "Iteration: 3360, Loss: 0.041940\n",
            "Iteration: 3370, Loss: 0.041790\n",
            "Iteration: 3380, Loss: 0.041679\n",
            "Iteration: 3390, Loss: 0.041474\n",
            "Iteration: 3400, Loss: 0.041283\n",
            "Iteration: 3410, Loss: 0.041124\n",
            "Iteration: 3420, Loss: 0.041003\n",
            "Iteration: 3430, Loss: 0.040884\n",
            "Iteration: 3440, Loss: 0.040755\n",
            "Iteration: 3450, Loss: 0.040666\n",
            "Iteration: 3460, Loss: 0.040608\n",
            "Iteration: 3470, Loss: 0.040543\n",
            "Iteration: 3480, Loss: 0.040499\n",
            "Iteration: 3490, Loss: 0.040439\n",
            "Iteration: 3500, Loss: 0.040321\n",
            "Iteration: 3510, Loss: 0.040249\n",
            "Iteration: 3520, Loss: 0.040123\n",
            "Iteration: 3530, Loss: 0.039883\n",
            "Iteration: 3540, Loss: 0.039714\n",
            "Iteration: 3550, Loss: 0.039477\n",
            "Iteration: 3560, Loss: 0.039380\n",
            "Iteration: 3570, Loss: 0.039088\n",
            "Iteration: 3580, Loss: 0.038990\n",
            "Iteration: 3590, Loss: 0.038814\n",
            "Iteration: 3600, Loss: 0.038668\n",
            "Iteration: 3610, Loss: 0.038541\n",
            "Iteration: 3620, Loss: 0.038415\n",
            "Iteration: 3630, Loss: 0.038101\n",
            "Iteration: 3640, Loss: 0.037640\n",
            "Iteration: 3650, Loss: 0.037420\n",
            "Iteration: 3660, Loss: 0.037156\n",
            "Iteration: 3670, Loss: 0.036799\n",
            "Iteration: 3680, Loss: 0.036611\n",
            "Iteration: 3690, Loss: 0.036509\n",
            "Iteration: 3700, Loss: 0.036307\n",
            "Iteration: 3710, Loss: 0.036027\n",
            "Iteration: 3720, Loss: 0.035742\n",
            "Iteration: 3730, Loss: 0.035607\n",
            "Iteration: 3740, Loss: 0.035429\n",
            "Iteration: 3750, Loss: 0.035321\n",
            "Iteration: 3760, Loss: 0.035263\n",
            "Iteration: 3770, Loss: 0.035131\n",
            "Iteration: 3780, Loss: 0.034849\n",
            "Iteration: 3790, Loss: 0.034721\n",
            "Iteration: 3800, Loss: 0.034480\n",
            "Iteration: 3810, Loss: 0.034318\n",
            "Iteration: 3820, Loss: 0.034202\n",
            "Iteration: 3830, Loss: 0.034046\n",
            "Iteration: 3840, Loss: 0.033753\n",
            "Iteration: 3850, Loss: 0.033402\n",
            "Iteration: 3860, Loss: 0.033145\n",
            "Iteration: 3870, Loss: 0.032956\n",
            "Iteration: 3880, Loss: 0.032867\n",
            "Iteration: 3890, Loss: 0.032725\n",
            "Iteration: 3900, Loss: 0.032524\n",
            "Iteration: 3910, Loss: 0.032269\n",
            "Iteration: 3920, Loss: 0.032186\n",
            "Iteration: 3930, Loss: 0.031969\n",
            "Iteration: 3940, Loss: 0.031871\n",
            "Iteration: 3950, Loss: 0.031739\n",
            "Iteration: 3960, Loss: 0.031591\n",
            "Iteration: 3970, Loss: 0.031537\n",
            "Iteration: 3980, Loss: 0.031408\n",
            "Iteration: 3990, Loss: 0.031210\n",
            "Iteration: 4000, Loss: 0.031040\n",
            "Iteration: 4010, Loss: 0.030946\n",
            "Iteration: 4020, Loss: 0.030811\n",
            "Iteration: 4030, Loss: 0.030616\n",
            "Iteration: 4040, Loss: 0.030432\n",
            "Iteration: 4050, Loss: 0.030339\n",
            "Iteration: 4060, Loss: 0.030238\n",
            "Iteration: 4070, Loss: 0.030070\n",
            "Iteration: 4080, Loss: 0.029970\n",
            "Iteration: 4090, Loss: 0.029792\n",
            "Iteration: 4100, Loss: 0.029605\n",
            "Iteration: 4110, Loss: 0.029434\n",
            "Iteration: 4120, Loss: 0.029352\n",
            "Iteration: 4130, Loss: 0.029278\n",
            "Iteration: 4140, Loss: 0.029079\n",
            "Iteration: 4150, Loss: 0.029013\n",
            "Iteration: 4160, Loss: 0.028901\n",
            "Iteration: 4170, Loss: 0.028605\n",
            "Iteration: 4180, Loss: 0.028483\n",
            "Iteration: 4190, Loss: 0.028349\n",
            "Iteration: 4200, Loss: 0.028258\n",
            "Iteration: 4210, Loss: 0.028220\n",
            "Iteration: 4220, Loss: 0.028114\n",
            "Iteration: 4230, Loss: 0.027998\n",
            "Iteration: 4240, Loss: 0.027816\n",
            "Iteration: 4250, Loss: 0.027679\n",
            "Iteration: 4260, Loss: 0.027424\n",
            "Iteration: 4270, Loss: 0.027288\n",
            "Iteration: 4280, Loss: 0.027085\n",
            "Iteration: 4290, Loss: 0.026936\n",
            "Iteration: 4300, Loss: 0.026762\n",
            "Iteration: 4310, Loss: 0.026588\n",
            "Iteration: 4320, Loss: 0.026461\n",
            "Iteration: 4330, Loss: 0.026102\n",
            "Iteration: 4340, Loss: 0.025817\n",
            "Iteration: 4350, Loss: 0.025662\n",
            "Iteration: 4360, Loss: 0.025503\n",
            "Iteration: 4370, Loss: 0.025140\n",
            "Iteration: 4380, Loss: 0.024873\n",
            "Iteration: 4390, Loss: 0.024488\n",
            "Iteration: 4400, Loss: 0.023975\n",
            "Iteration: 4410, Loss: 0.023812\n",
            "Iteration: 4420, Loss: 0.023607\n",
            "Iteration: 4430, Loss: 0.023515\n",
            "Iteration: 4440, Loss: 0.023321\n",
            "Iteration: 4450, Loss: 0.023207\n",
            "Iteration: 4460, Loss: 0.023044\n",
            "Iteration: 4470, Loss: 0.022797\n",
            "Iteration: 4480, Loss: 0.022710\n",
            "Iteration: 4490, Loss: 0.022679\n",
            "Iteration: 4500, Loss: 0.022564\n",
            "Iteration: 4510, Loss: 0.022448\n",
            "Iteration: 4520, Loss: 0.022393\n",
            "Iteration: 4530, Loss: 0.022240\n",
            "Iteration: 4540, Loss: 0.022122\n",
            "Iteration: 4550, Loss: 0.022005\n",
            "Iteration: 4560, Loss: 0.021899\n",
            "Iteration: 4570, Loss: 0.021776\n",
            "Iteration: 4580, Loss: 0.021683\n",
            "Iteration: 4590, Loss: 0.021573\n",
            "Iteration: 4600, Loss: 0.021440\n",
            "Iteration: 4610, Loss: 0.021310\n",
            "Iteration: 4620, Loss: 0.021228\n",
            "Iteration: 4630, Loss: 0.021119\n",
            "Iteration: 4640, Loss: 0.020919\n",
            "Iteration: 4650, Loss: 0.020841\n",
            "Iteration: 4660, Loss: 0.020723\n",
            "Iteration: 4670, Loss: 0.020661\n",
            "Iteration: 4680, Loss: 0.020587\n",
            "Iteration: 4690, Loss: 0.020459\n",
            "Iteration: 4700, Loss: 0.020402\n",
            "Iteration: 4710, Loss: 0.020291\n",
            "Iteration: 4720, Loss: 0.020181\n",
            "Iteration: 4730, Loss: 0.020146\n",
            "Iteration: 4740, Loss: 0.020064\n",
            "Iteration: 4750, Loss: 0.019946\n",
            "Iteration: 4760, Loss: 0.019834\n",
            "Iteration: 4770, Loss: 0.019778\n",
            "Iteration: 4780, Loss: 0.019707\n",
            "Iteration: 4790, Loss: 0.019645\n",
            "Iteration: 4800, Loss: 0.019565\n",
            "Iteration: 4810, Loss: 0.019428\n",
            "Iteration: 4820, Loss: 0.019369\n",
            "Iteration: 4830, Loss: 0.019270\n",
            "Iteration: 4840, Loss: 0.019216\n",
            "Iteration: 4850, Loss: 0.019127\n",
            "Iteration: 4860, Loss: 0.018934\n",
            "Iteration: 4870, Loss: 0.018782\n",
            "Iteration: 4880, Loss: 0.018708\n",
            "Iteration: 4890, Loss: 0.018602\n",
            "Iteration: 4900, Loss: 0.018554\n",
            "Iteration: 4910, Loss: 0.018508\n",
            "Iteration: 4920, Loss: 0.018449\n",
            "Iteration: 4930, Loss: 0.018323\n",
            "Iteration: 4940, Loss: 0.018263\n",
            "Iteration: 4950, Loss: 0.018231\n",
            "Iteration: 4960, Loss: 0.018191\n",
            "Iteration: 4970, Loss: 0.018088\n",
            "Iteration: 4980, Loss: 0.018037\n",
            "Iteration: 4990, Loss: 0.018006\n",
            "Iteration: 5000, Loss: 0.017974\n",
            "Iteration: 5010, Loss: 0.017943\n",
            "Iteration: 5020, Loss: 0.017871\n",
            "Iteration: 5030, Loss: 0.017783\n",
            "Iteration: 5040, Loss: 0.017692\n",
            "Iteration: 5050, Loss: 0.017609\n",
            "Iteration: 5060, Loss: 0.017511\n",
            "Iteration: 5070, Loss: 0.017415\n",
            "Iteration: 5080, Loss: 0.017341\n",
            "Iteration: 5090, Loss: 0.017252\n",
            "Iteration: 5100, Loss: 0.017154\n",
            "Iteration: 5110, Loss: 0.017090\n",
            "Iteration: 5120, Loss: 0.016977\n",
            "Iteration: 5130, Loss: 0.016915\n",
            "Iteration: 5140, Loss: 0.016896\n",
            "Iteration: 5150, Loss: 0.016867\n",
            "Iteration: 5160, Loss: 0.016803\n",
            "Iteration: 5170, Loss: 0.016739\n",
            "Iteration: 5180, Loss: 0.016692\n",
            "Iteration: 5190, Loss: 0.016673\n",
            "Iteration: 5200, Loss: 0.016637\n",
            "Iteration: 5210, Loss: 0.016596\n",
            "Iteration: 5220, Loss: 0.016534\n",
            "Iteration: 5230, Loss: 0.016488\n",
            "Iteration: 5240, Loss: 0.016467\n",
            "Iteration: 5250, Loss: 0.016425\n",
            "Iteration: 5260, Loss: 0.016402\n",
            "Iteration: 5270, Loss: 0.016386\n",
            "Iteration: 5280, Loss: 0.016347\n",
            "Iteration: 5290, Loss: 0.016291\n",
            "Iteration: 5300, Loss: 0.016272\n",
            "Iteration: 5310, Loss: 0.016235\n",
            "Iteration: 5320, Loss: 0.016196\n",
            "Iteration: 5330, Loss: 0.016143\n",
            "Iteration: 5340, Loss: 0.016099\n",
            "Iteration: 5350, Loss: 0.016082\n",
            "Iteration: 5360, Loss: 0.016061\n",
            "Iteration: 5370, Loss: 0.015987\n",
            "Iteration: 5380, Loss: 0.015951\n",
            "Iteration: 5390, Loss: 0.015915\n",
            "Iteration: 5400, Loss: 0.015894\n",
            "Iteration: 5410, Loss: 0.015822\n",
            "Iteration: 5420, Loss: 0.015777\n",
            "Iteration: 5430, Loss: 0.015761\n",
            "Iteration: 5440, Loss: 0.015745\n",
            "Iteration: 5450, Loss: 0.015732\n",
            "Iteration: 5460, Loss: 0.015704\n",
            "Iteration: 5470, Loss: 0.015687\n",
            "Iteration: 5480, Loss: 0.015668\n",
            "Iteration: 5490, Loss: 0.015628\n",
            "Iteration: 5500, Loss: 0.015565\n",
            "Iteration: 5510, Loss: 0.015515\n",
            "Iteration: 5520, Loss: 0.015496\n",
            "Iteration: 5530, Loss: 0.015464\n",
            "Iteration: 5540, Loss: 0.015426\n",
            "Iteration: 5550, Loss: 0.015378\n",
            "Iteration: 5560, Loss: 0.015356\n",
            "Iteration: 5570, Loss: 0.015328\n",
            "Iteration: 5580, Loss: 0.015271\n",
            "Iteration: 5590, Loss: 0.015210\n",
            "Iteration: 5600, Loss: 0.015189\n",
            "Iteration: 5610, Loss: 0.015126\n",
            "Iteration: 5620, Loss: 0.015071\n",
            "Iteration: 5630, Loss: 0.015048\n",
            "Iteration: 5640, Loss: 0.015003\n",
            "Iteration: 5650, Loss: 0.014942\n",
            "Iteration: 5660, Loss: 0.014869\n",
            "Iteration: 5670, Loss: 0.014797\n",
            "Iteration: 5680, Loss: 0.014732\n",
            "Iteration: 5690, Loss: 0.014670\n",
            "Iteration: 5700, Loss: 0.014658\n",
            "Iteration: 5710, Loss: 0.014636\n",
            "Iteration: 5720, Loss: 0.014619\n",
            "Iteration: 5730, Loss: 0.014595\n",
            "Iteration: 5740, Loss: 0.014573\n",
            "Iteration: 5750, Loss: 0.014544\n",
            "Iteration: 5760, Loss: 0.014527\n",
            "Iteration: 5770, Loss: 0.014507\n",
            "Iteration: 5780, Loss: 0.014482\n",
            "Iteration: 5790, Loss: 0.014416\n",
            "Iteration: 5800, Loss: 0.014378\n",
            "Iteration: 5810, Loss: 0.014318\n",
            "Iteration: 5820, Loss: 0.014291\n",
            "Iteration: 5830, Loss: 0.014266\n",
            "Iteration: 5840, Loss: 0.014224\n",
            "Iteration: 5850, Loss: 0.014201\n",
            "Iteration: 5860, Loss: 0.014148\n",
            "Iteration: 5870, Loss: 0.014120\n",
            "Iteration: 5880, Loss: 0.014103\n",
            "Iteration: 5890, Loss: 0.014088\n",
            "Iteration: 5900, Loss: 0.014064\n",
            "Iteration: 5910, Loss: 0.014042\n",
            "Iteration: 5920, Loss: 0.013989\n",
            "Iteration: 5930, Loss: 0.013956\n",
            "Iteration: 5940, Loss: 0.013934\n",
            "Iteration: 5950, Loss: 0.013918\n",
            "Iteration: 5960, Loss: 0.013902\n",
            "Iteration: 5970, Loss: 0.013872\n",
            "Iteration: 5980, Loss: 0.013825\n",
            "Iteration: 5990, Loss: 0.013792\n",
            "Iteration: 6000, Loss: 0.013742\n",
            "Iteration: 6010, Loss: 0.013713\n",
            "Iteration: 6020, Loss: 0.013667\n",
            "Iteration: 6030, Loss: 0.013629\n",
            "Iteration: 6040, Loss: 0.013613\n",
            "Iteration: 6050, Loss: 0.013583\n",
            "Iteration: 6060, Loss: 0.013536\n",
            "Iteration: 6070, Loss: 0.013496\n",
            "Iteration: 6080, Loss: 0.013473\n",
            "Iteration: 6090, Loss: 0.013429\n",
            "Iteration: 6100, Loss: 0.013380\n",
            "Iteration: 6110, Loss: 0.013349\n",
            "Iteration: 6120, Loss: 0.013321\n",
            "Iteration: 6130, Loss: 0.013307\n",
            "Iteration: 6140, Loss: 0.013295\n",
            "Iteration: 6150, Loss: 0.013261\n",
            "Iteration: 6160, Loss: 0.013233\n",
            "Iteration: 6170, Loss: 0.013198\n",
            "Iteration: 6180, Loss: 0.013171\n",
            "Iteration: 6190, Loss: 0.013127\n",
            "Iteration: 6200, Loss: 0.013082\n",
            "Iteration: 6210, Loss: 0.013040\n",
            "Iteration: 6220, Loss: 0.013010\n",
            "Iteration: 6230, Loss: 0.012992\n",
            "Iteration: 6240, Loss: 0.012970\n",
            "Iteration: 6250, Loss: 0.012940\n",
            "Iteration: 6260, Loss: 0.012908\n",
            "Iteration: 6270, Loss: 0.012877\n",
            "Iteration: 6280, Loss: 0.012848\n",
            "Iteration: 6290, Loss: 0.012818\n",
            "Iteration: 6300, Loss: 0.012797\n",
            "Iteration: 6310, Loss: 0.012716\n",
            "Iteration: 6320, Loss: 0.012688\n",
            "Iteration: 6330, Loss: 0.012661\n",
            "Iteration: 6340, Loss: 0.012627\n",
            "Iteration: 6350, Loss: 0.012572\n",
            "Iteration: 6360, Loss: 0.012535\n",
            "Iteration: 6370, Loss: 0.012512\n",
            "Iteration: 6380, Loss: 0.012495\n",
            "Iteration: 6390, Loss: 0.012483\n",
            "Iteration: 6400, Loss: 0.012451\n",
            "Iteration: 6410, Loss: 0.012425\n",
            "Iteration: 6420, Loss: 0.012386\n",
            "Iteration: 6430, Loss: 0.012351\n",
            "Iteration: 6440, Loss: 0.012314\n",
            "Iteration: 6450, Loss: 0.012262\n",
            "Iteration: 6460, Loss: 0.012214\n",
            "Iteration: 6470, Loss: 0.012197\n",
            "Iteration: 6480, Loss: 0.012161\n",
            "Iteration: 6490, Loss: 0.012130\n",
            "Iteration: 6500, Loss: 0.012103\n",
            "Iteration: 6510, Loss: 0.012074\n",
            "Iteration: 6520, Loss: 0.012031\n",
            "Iteration: 6530, Loss: 0.011990\n",
            "Iteration: 6540, Loss: 0.011974\n",
            "Iteration: 6550, Loss: 0.011951\n",
            "Iteration: 6560, Loss: 0.011916\n",
            "Iteration: 6570, Loss: 0.011890\n",
            "Iteration: 6580, Loss: 0.011863\n",
            "Iteration: 6590, Loss: 0.011830\n",
            "Iteration: 6600, Loss: 0.011809\n",
            "Iteration: 6610, Loss: 0.011790\n",
            "Iteration: 6620, Loss: 0.011764\n",
            "Iteration: 6630, Loss: 0.011746\n",
            "Iteration: 6640, Loss: 0.011721\n",
            "Iteration: 6650, Loss: 0.011698\n",
            "Iteration: 6660, Loss: 0.011668\n",
            "Iteration: 6670, Loss: 0.011644\n",
            "Iteration: 6680, Loss: 0.011623\n",
            "Iteration: 6690, Loss: 0.011598\n",
            "Iteration: 6700, Loss: 0.011549\n",
            "Iteration: 6710, Loss: 0.011510\n",
            "Iteration: 6720, Loss: 0.011490\n",
            "Iteration: 6730, Loss: 0.011466\n",
            "Iteration: 6740, Loss: 0.011430\n",
            "Iteration: 6750, Loss: 0.011412\n",
            "Iteration: 6760, Loss: 0.011394\n",
            "Iteration: 6770, Loss: 0.011374\n",
            "Iteration: 6780, Loss: 0.011348\n",
            "Iteration: 6790, Loss: 0.011334\n",
            "Iteration: 6800, Loss: 0.011326\n",
            "Iteration: 6810, Loss: 0.011313\n",
            "Iteration: 6820, Loss: 0.011293\n",
            "Iteration: 6830, Loss: 0.011278\n",
            "Iteration: 6840, Loss: 0.011261\n",
            "Iteration: 6850, Loss: 0.011231\n",
            "Iteration: 6860, Loss: 0.011208\n",
            "Iteration: 6870, Loss: 0.011183\n",
            "Iteration: 6880, Loss: 0.011163\n",
            "Iteration: 6890, Loss: 0.011153\n",
            "Iteration: 6900, Loss: 0.011123\n",
            "Iteration: 6910, Loss: 0.011081\n",
            "Iteration: 6920, Loss: 0.011044\n",
            "Iteration: 6930, Loss: 0.011021\n",
            "Iteration: 6940, Loss: 0.011005\n",
            "Iteration: 6950, Loss: 0.010990\n",
            "Iteration: 6960, Loss: 0.010969\n",
            "Iteration: 6970, Loss: 0.010949\n",
            "Iteration: 6980, Loss: 0.010926\n",
            "Iteration: 6990, Loss: 0.010914\n",
            "Iteration: 7000, Loss: 0.010876\n",
            "Iteration: 7010, Loss: 0.010810\n",
            "Iteration: 7020, Loss: 0.010796\n",
            "Iteration: 7030, Loss: 0.010776\n",
            "Iteration: 7040, Loss: 0.010749\n",
            "Iteration: 7050, Loss: 0.010701\n",
            "Iteration: 7060, Loss: 0.010666\n",
            "Iteration: 7070, Loss: 0.010648\n",
            "Iteration: 7080, Loss: 0.010633\n",
            "Iteration: 7090, Loss: 0.010588\n",
            "Iteration: 7100, Loss: 0.010550\n",
            "Iteration: 7110, Loss: 0.010512\n",
            "Iteration: 7120, Loss: 0.010493\n",
            "Iteration: 7130, Loss: 0.010473\n",
            "Iteration: 7140, Loss: 0.010453\n",
            "Iteration: 7150, Loss: 0.010418\n",
            "Iteration: 7160, Loss: 0.010401\n",
            "Iteration: 7170, Loss: 0.010375\n",
            "Iteration: 7180, Loss: 0.010357\n",
            "Iteration: 7190, Loss: 0.010338\n",
            "Iteration: 7200, Loss: 0.010319\n",
            "Iteration: 7210, Loss: 0.010300\n",
            "Iteration: 7220, Loss: 0.010281\n",
            "Iteration: 7230, Loss: 0.010262\n",
            "Iteration: 7240, Loss: 0.010244\n",
            "Iteration: 7250, Loss: 0.010228\n",
            "Iteration: 7260, Loss: 0.010202\n",
            "Iteration: 7270, Loss: 0.010182\n",
            "Iteration: 7280, Loss: 0.010169\n",
            "Iteration: 7290, Loss: 0.010161\n",
            "Iteration: 7300, Loss: 0.010140\n",
            "Iteration: 7310, Loss: 0.010108\n",
            "Iteration: 7320, Loss: 0.010092\n",
            "Iteration: 7330, Loss: 0.010070\n",
            "Iteration: 7340, Loss: 0.010051\n",
            "Iteration: 7350, Loss: 0.010021\n",
            "Iteration: 7360, Loss: 0.009993\n",
            "Iteration: 7370, Loss: 0.009980\n",
            "Iteration: 7380, Loss: 0.009966\n",
            "Iteration: 7390, Loss: 0.009942\n",
            "Iteration: 7400, Loss: 0.009930\n",
            "Iteration: 7410, Loss: 0.009918\n",
            "Iteration: 7420, Loss: 0.009899\n",
            "Iteration: 7430, Loss: 0.009864\n",
            "Iteration: 7440, Loss: 0.009840\n",
            "Iteration: 7450, Loss: 0.009824\n",
            "Iteration: 7460, Loss: 0.009810\n",
            "Iteration: 7470, Loss: 0.009797\n",
            "Iteration: 7480, Loss: 0.009777\n",
            "Iteration: 7490, Loss: 0.009749\n",
            "Iteration: 7500, Loss: 0.009730\n",
            "Iteration: 7510, Loss: 0.009719\n",
            "Iteration: 7520, Loss: 0.009704\n",
            "Iteration: 7530, Loss: 0.009668\n",
            "Iteration: 7540, Loss: 0.009641\n",
            "Iteration: 7550, Loss: 0.009625\n",
            "Iteration: 7560, Loss: 0.009604\n",
            "Iteration: 7570, Loss: 0.009589\n",
            "Iteration: 7580, Loss: 0.009575\n",
            "Iteration: 7590, Loss: 0.009560\n",
            "Iteration: 7600, Loss: 0.009531\n",
            "Iteration: 7610, Loss: 0.009516\n",
            "Iteration: 7620, Loss: 0.009499\n",
            "Iteration: 7630, Loss: 0.009483\n",
            "Iteration: 7640, Loss: 0.009457\n",
            "Iteration: 7650, Loss: 0.009434\n",
            "Iteration: 7660, Loss: 0.009424\n",
            "Iteration: 7670, Loss: 0.009417\n",
            "Iteration: 7680, Loss: 0.009404\n",
            "Iteration: 7690, Loss: 0.009388\n",
            "Iteration: 7700, Loss: 0.009365\n",
            "Iteration: 7710, Loss: 0.009354\n",
            "Iteration: 7720, Loss: 0.009343\n",
            "Iteration: 7730, Loss: 0.009307\n",
            "Iteration: 7740, Loss: 0.009276\n",
            "Iteration: 7750, Loss: 0.009253\n",
            "Iteration: 7760, Loss: 0.009233\n",
            "Iteration: 7770, Loss: 0.009214\n",
            "Iteration: 7780, Loss: 0.009205\n",
            "Iteration: 7790, Loss: 0.009188\n",
            "Iteration: 7800, Loss: 0.009170\n",
            "Iteration: 7810, Loss: 0.009143\n",
            "Iteration: 7820, Loss: 0.009128\n",
            "Iteration: 7830, Loss: 0.009104\n",
            "Iteration: 7840, Loss: 0.009082\n",
            "Iteration: 7850, Loss: 0.009060\n",
            "Iteration: 7860, Loss: 0.009041\n",
            "Iteration: 7870, Loss: 0.009028\n",
            "Iteration: 7880, Loss: 0.009019\n",
            "Iteration: 7890, Loss: 0.009009\n",
            "Iteration: 7900, Loss: 0.008997\n",
            "Iteration: 7910, Loss: 0.008980\n",
            "Iteration: 7920, Loss: 0.008964\n",
            "Iteration: 7930, Loss: 0.008942\n",
            "Iteration: 7940, Loss: 0.008926\n",
            "Iteration: 7950, Loss: 0.008899\n",
            "Iteration: 7960, Loss: 0.008885\n",
            "Iteration: 7970, Loss: 0.008874\n",
            "Iteration: 7980, Loss: 0.008854\n",
            "Iteration: 7990, Loss: 0.008837\n",
            "Iteration: 8000, Loss: 0.008813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model for inference\n",
        "pinn_transient = TransientNavierStokes(x_train, y_train, t_train, u_train, v_train)\n",
        "pinn_transient.net.load_state_dict(torch.load('transient_model.pt'))\n",
        "pinn_transient.net.eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK0N6gwgKJZ7",
        "outputId": "81080f44-4832-49b3-a6e7-dffae4871ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-7f8a57dda9e9>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pinn_transient.net.load_state_dict(torch.load('transient_model.pt'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3, out_features=20, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (3): Tanh()\n",
              "  (4): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (5): Tanh()\n",
              "  (6): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (7): Tanh()\n",
              "  (8): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (9): Tanh()\n",
              "  (10): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (11): Tanh()\n",
              "  (12): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (13): Tanh()\n",
              "  (14): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (15): Tanh()\n",
              "  (16): Linear(in_features=20, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(0, 1.1, 100)  # Example range for x coordinates\n",
        "y_test = np.linspace(0, 0.41, 50)  # Example range for y coordinates\n",
        "t_test = np.array([0.1])\n",
        "\n",
        "\n",
        "x_test, y_test = np.meshgrid(x_test, y_test)\n",
        "x_test = x_test.flatten()[:, None]\n",
        "y_test = y_test.flatten()[:, None]\n",
        "t_test = np.ones_like(x_test) * t_test  # Same time for all test points\n",
        "\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32, requires_grad=True)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, requires_grad=True)\n",
        "t_test_tensor = torch.tensor(t_test, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "\n",
        "u_out, v_out, p_out, _, _ = pinn_transient.function(x_test_tensor, y_test_tensor, t_test_tensor)\n"
      ],
      "metadata": {
        "id": "Dxo2BWlpKLHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def animate(i):\n",
        "\n",
        "    t_test = np.ones_like(x_test) * t_star[i]\n",
        "    t_test_tensor = torch.tensor(t_test, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        u_out, v_out, p_out, _, _ = pinn_transient.function(x_test_tensor, y_test_tensor, t_test_tensor)\n",
        "\n",
        "    u_plot = u_out.data.cpu().numpy().reshape((50, 100))\n",
        "    v_plot = v_out.data.cpu().numpy().reshape((50, 100))\n",
        "    p_plot = p_out.data.cpu().numpy().reshape((50, 100))\n",
        "\n",
        "    u_contour = ax[0].contourf(u_plot, levels=30, cmap='jet')\n",
        "    ax[0].set_title('u Velocity Field')\n",
        "    ax[0].set_xlabel('x')\n",
        "    ax[0].set_ylabel('y')\n",
        "    fig.colorbar(u_contour, ax=ax[0])\n",
        "\n",
        "    v_contour = ax[1].contourf(v_plot, levels=30, cmap='jet')\n",
        "    ax[1].set_title('v Velocity Field')\n",
        "    ax[1].set_xlabel('x')\n",
        "    ax[1].set_ylabel('y')\n",
        "    fig.colorbar(v_contour, ax=ax[1])\n",
        "\n",
        "    p_contour = ax[2].contourf(p_plot, levels=30, cmap='jet')\n",
        "    ax[2].set_title('Pressure Field')\n",
        "    ax[2].set_xlabel('x')\n",
        "    ax[2].set_ylabel('y')\n",
        "    fig.colorbar(p_contour, ax=ax[2])\n",
        "\n",
        "    plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "HSA5ZKe1KN9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}